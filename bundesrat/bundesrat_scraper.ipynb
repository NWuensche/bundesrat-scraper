{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from urllib.parse import urlsplit, urlunsplit, parse_qs, urlencode\n",
    "\n",
    "import requests\n",
    "from lxml import html as etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CACHE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "UA = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36'\n",
    "BASE_HOST = 'www.bundesrat.de'\n",
    "BASE_URL = 'https://www.bundesrat.de/'\n",
    "START_PATH = 'DE/plenum/to-plenum/to-plenum-node.html'\n",
    "TO_URL = 'https://www.bundesrat.de/SharedDocs/TO/{num}/to-node.html'\n",
    "TOP_URL = 'https://www.bundesrat.de/SharedDocs/TO/{num}/tops/{top}.html?view=render[StandardBody]'\n",
    "\n",
    "SESSION_NUM_RE = re.compile(r'(\\d+)\\. Plenarsitzung')\n",
    "DATE_RE = re.compile(r'(\\d{2}\\.\\d{2}\\.\\d{4})')\n",
    "TIME_RE = re.compile(r'(\\d{2}:\\d{2}) Uhr')\n",
    "TOP_NUMBER_RE = re.compile(r'TOP (\\d+[a-z]?)')\n",
    "TOP_SIMPLE_NUMBER_RE = re.compile(r'TOP ([a-z]?)')\n",
    "# BR 3/18(B)  Beschlussdrucksache  (PDF, 112KB)\n",
    "DOC_TITLE = re.compile(r'(?:(?P<doc_id>[A-Z]{2,}.*)\\s{2})?(?P<doc_kind>.*)\\s{2}\\((?P<doc_filetype>[A-Z]{3,}),?\\s+(?P<doc_filesize>\\d+[KMGT]B)\\)')\n",
    "\n",
    "START_URL_ARCHIVE=\"DE/service/archiv/to-archiv/to-archiv-node.html\"\n",
    "SESSION_NUM_RE_ARCHIVE = re.compile(r'.* \\| (\\d+)\\. .*')\n",
    "DATE_RE_ARCHIVE = re.compile(r'(\\d{2}\\.\\d{2}\\.\\d{4})')\n",
    "\n",
    "STATES = [\n",
    "    'Schleswig-Holstein',\n",
    "    'Hamburg',\n",
    "    'Mecklenburg-Vorpommern',\n",
    "    'Niedersachsen',\n",
    "    'Bremen',\n",
    "    'Berlin',\n",
    "    'Brandenburg',\n",
    "    'Sachsen',\n",
    "    'Sachsen-Anhalt',\n",
    "    'Thüringen',\n",
    "    'Hessen',\n",
    "    'Nordrhein-Westfalen',\n",
    "    'Rheinland-Pfalz',\n",
    "    'Saarland',\n",
    "    'Baden-Württemberg',\n",
    "    'Bayern'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CACHE:\n",
    "    os.makedirs('./_cache', exist_ok=True)\n",
    "\n",
    "def get(url):\n",
    "    if USE_CACHE:\n",
    "        filename = url.split('bundesrat.de/')[1].split('?')[0].replace('/', '_')\n",
    "        filename = os.path.join('./_cache', filename)\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename) as f:\n",
    "                return f.read()\n",
    "    text = requests.get(url, headers={'User-Agent': UA}).text\n",
    "    if USE_CACHE:\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sessions_this_year():\n",
    "    root = etree.fromstring(get(BASE_URL + START_PATH))\n",
    "    for table in root.xpath('.//table'):\n",
    "        for row in table.xpath('.//tr'):\n",
    "            cols = row.xpath('./td')\n",
    "            if len(cols) != 2:\n",
    "                # Wrong table\n",
    "                break\n",
    "            path = cols[0].xpath('.//a')[0].attrib['href']\n",
    "            path = path.split(';jsessionid=')[0]\n",
    "            # path is relative to base url\n",
    "            url = BASE_URL + path\n",
    "            title = cols[0].text_content()\n",
    "            session_num = SESSION_NUM_RE.search(title)\n",
    "            if session_num is None:\n",
    "                continue\n",
    "            num = int(session_num.group(1))\n",
    "            date_time = cols[1].text_content()\n",
    "            date = DATE_RE.search(date_time).group(1)\n",
    "            time = TIME_RE.search(date_time).group(1)\n",
    "            timestamp = datetime.strptime('{} {}'.format(date, time), '%d.%m.%Y %H:%M')\n",
    "            yield {\n",
    "                'number': num,\n",
    "                'timestamp': timestamp.isoformat(),\n",
    "                'url': url\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sessions_archive():\n",
    "    root = etree.fromstring(get(BASE_URL + START_URL_ARCHIVE))\n",
    "    for table in root.xpath('.//ul[@class=\"link-list\"]'):\n",
    "        for row in table.xpath('.//li'):\n",
    "            path = row.xpath('.//a')[0].attrib['href']\n",
    "            path = path.split(';jsessionid=')[0]\n",
    "            # path is relative to base url\n",
    "            url = BASE_URL + path\n",
    "            title = row.text_content()\n",
    "            session_num = SESSION_NUM_RE_ARCHIVE.search(title)\n",
    "            if session_num is None:\n",
    "                continue\n",
    "            num = int(session_num.group(1))\n",
    "            details = etree.fromstring(get(BASE_URL + path)) #In archive, date and time are visible only on the detailed page of a meeting\n",
    "            \n",
    "            if num == 955: # they used a different website layout for this meeting\n",
    "                tableD = details.xpath('//*[@id=\"super-content\"]/main/div[1]/article/div/div[2]/div/p[1]')\n",
    "                TIME_RE_ARCHIVE = re.compile(r'(\\d{2}:\\d{2})')\n",
    "            else:\n",
    "                tableD = details.xpath('/html/body/div[2]/div[1]/div/div[2]/main/div[1]/div[1]/div/div/div/h1/em')\n",
    "                TIME_RE_ARCHIVE = re.compile(r'Beginn: (\\d{2}:\\d{2}|\\d{1}:\\d{2})') # they used only one digit for 9 o'clock\n",
    "\n",
    "            date_time = str(etree.tostring(tableD[0], pretty_print=True))\n",
    "            date = DATE_RE_ARCHIVE.search(date_time).group(1)\n",
    "            time = TIME_RE_ARCHIVE.search(date_time).group(1)\n",
    "            timestamp = datetime.strptime('{} {}'.format(date, time), '%d.%m.%Y %H:%M')\n",
    "            yield {\n",
    "                'number': num,\n",
    "                'timestamp': timestamp.isoformat(),\n",
    "                'url': url\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSESSIONID_PATH = ';jsessionid='\n",
    "\n",
    "def fix_url(url):\n",
    "    result = urlsplit(url)\n",
    "    parts = list(result)\n",
    "    if result.scheme == '':\n",
    "        parts[0] = 'https'\n",
    "    if result.netloc == '':\n",
    "        parts[1] = BASE_HOST\n",
    "    if JSESSIONID_PATH in result.path:\n",
    "        # Remove session id\n",
    "        parts[2] = result.path.split(JSESSIONID_PATH)[0]\n",
    "\n",
    "    if parts[1] == BASE_HOST:\n",
    "        # Remove useless 'nn' query param\n",
    "        qs = parse_qs(result.query)\n",
    "        qs.pop('nn', None)\n",
    "        parts[3] = urlencode(qs, doseq=True)\n",
    "    return urlunsplit(parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.bundesrat.de/DE/plenum/bundesrat-kompakt/18/964/964-pk.html#top-2a'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_url('DE/plenum/bundesrat-kompakt/18/964/964-pk.html;jsessionid=93D6631A016E47B9E032236FE39F9ED1.2_cid382?nn=4352766#top-2a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extract(elements):\n",
    "    return '\\n'.join(e.text_content() for e in elements).strip()\n",
    "\n",
    "\n",
    "def press_release(elements):\n",
    "    data = {}\n",
    "    for el in elements:\n",
    "        if el.tag == 'ul':\n",
    "            data['links'] = list(link_list_extract(el))\n",
    "        else:\n",
    "            data.update(link_extract(el))\n",
    "    return data\n",
    "\n",
    "def extract_links(element, transform=None):\n",
    "    a_els = element.xpath('.//a')\n",
    "    for a in a_els:\n",
    "        data = {\n",
    "            'title': a.text_content(),\n",
    "            'url': fix_url(a.attrib['href'])\n",
    "        }\n",
    "        if transform:\n",
    "            data = transform(data)\n",
    "        yield data\n",
    "        \n",
    "def link_list_extract(elements, transform=None):\n",
    "    for el in elements:\n",
    "        yield from extract_links(el, transform=transform)\n",
    "    \n",
    "def link_extract(elements):\n",
    "    return list(link_list_extract(elements))\n",
    "\n",
    "FILESIZE_PREFIX = {\n",
    "    'K': 1024 ** 1,\n",
    "    'M': 1024 ** 2,\n",
    "    'G': 1024 ** 3,\n",
    "}\n",
    "\n",
    "def document_link_transform(data):\n",
    "    title = data['title']\n",
    "    match = DOC_TITLE.search(title)\n",
    "    if match is not None:\n",
    "        data.update(match.groupdict())\n",
    "        if data.get('doc_filesize'):\n",
    "            data['doc_filesize_bytes'] = int(data['doc_filesize'][:-2]) * FILESIZE_PREFIX[data['doc_filesize'][-2]]\n",
    "    return data\n",
    "\n",
    "def document_links(elements):\n",
    "    return list(link_list_extract(elements, transform=document_link_transform))\n",
    "\n",
    "def url_extract(element):\n",
    "    return [x['url'] for x in link_extract(element)]\n",
    "\n",
    "def get_states(text):\n",
    "    for state in STATES:\n",
    "        if state in text:\n",
    "            yield state\n",
    "\n",
    "def states_involved(els):\n",
    "    text = text_extract(els)\n",
    "    return {\n",
    "        'text': text,\n",
    "        'states': list(get_states(text))\n",
    "    }\n",
    "\n",
    "def get_committees(els):\n",
    "    committees = []\n",
    "    for el in els:\n",
    "        for abbr in el.xpath('./abbr'):\n",
    "            text = abbr.text_content()\n",
    "            if text == 'fdf':\n",
    "                committees[-1]['leading'] = True\n",
    "                continue\n",
    "            committees.append({\n",
    "                'name': abbr.attrib['title'],\n",
    "                'abbreviation': text,\n",
    "                'leading': False\n",
    "            })\n",
    "    return committees\n",
    "\n",
    "def related_tops(els):\n",
    "    for el in els:\n",
    "        if el.tag != 'ul':\n",
    "            continue\n",
    "        links = link_list_extract(el)\n",
    "        return [l['title'].replace('TOP ', '') for l in links]\n",
    "\n",
    "#It's possible that a person doesn't have a party\n",
    "def getParty(speech):\n",
    "    partyTag = speech.xpath('.//p[not(@class)]')\n",
    "    if len(partyTag) == 0: \n",
    "        return None\n",
    "    else:\n",
    "        return speech.xpath('.//p[not(@class)]')[0].text_content()\n",
    "\n",
    "\n",
    "def speech_parser(elements):\n",
    "    element = elements[0]\n",
    "    for speech in element.xpath('.//div[@class=\"rack-teaser\"]'):\n",
    "        data = {\n",
    "            'name': speech.xpath('.//h3')[0].text_content(),\n",
    "            'party': getParty(speech),\n",
    "            'url': fix_url(speech.xpath('.//a')[0].attrib['href'])   \n",
    "        }\n",
    "        try:\n",
    "            data['state'] = speech.xpath('.//p[@class=\"bundesland\"]')[0].text_content()\n",
    "        except IndexError:\n",
    "            pass\n",
    "        try:\n",
    "            data['ministry'] = speech.xpath('.//p[@class=\"ressort\"]')[0].text_content()\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        image = speech.xpath('.//img')\n",
    "        if image:\n",
    "            image = image[0]\n",
    "            data['image_url'] = fix_url(image.attrib['src'])\n",
    "            data['image_credit'] = image.attrib['title']\n",
    "        yield data\n",
    "        \n",
    "def speech_parser_list(elements):\n",
    "    return list(speech_parser(elements))\n",
    "\n",
    "    \n",
    "TOP_HEADINGS = {\n",
    "    'Beschlusstenor': 'beschlusstenor',\n",
    "    'BundesratKOMPAKT': 'press',\n",
    "    'Vorgang in DIP': 'dip',\n",
    "    'Drucksachen': 'documents',\n",
    "    'Länderbeteiligung': 'states_involved',\n",
    "    'Ausschusszuweisung': 'committee',\n",
    "    'Bemerkungen': 'notes',\n",
    "    'Gesetzeskategorie': 'law_category',\n",
    "}\n",
    "\n",
    "TOP_PARSERS = {\n",
    "    'notes': text_extract,\n",
    "    'beschlusstenor': text_extract,\n",
    "    'press': press_release,\n",
    "    'dip': url_extract,\n",
    "    'documents': document_links,\n",
    "    'states_involved': states_involved,\n",
    "    'committee': get_committees,\n",
    "    'related_tops': related_tops,\n",
    "    'links': document_links,\n",
    "    'speeches': speech_parser_list,\n",
    "    'law_category': text_extract,\n",
    "}\n",
    "\n",
    "def parse_top_detail(root):\n",
    "    heading_elements = defaultdict(list)\n",
    "    heading = None\n",
    "    for element in root.xpath('.//div[contains(@class, \"top-content-full\")]/*'):\n",
    "        if element.tag == 'h3':\n",
    "            title = element.text_content().strip()\n",
    "            heading = TOP_HEADINGS[title]\n",
    "        elif element.tag == 'div' and element.attrib['class'] == 'related-tops':\n",
    "            heading = 'related_tops'\n",
    "            heading_elements[heading].append(element)\n",
    "        elif element.tag == 'div' and element.attrib['class'] == 'ts-members':\n",
    "            heading = 'speeches'\n",
    "            heading_elements[heading].append(element)\n",
    "        elif element.tag == 'ul' and element.attrib['class'] == 'link-list doc-list' and heading != 'documents':\n",
    "            heading = 'links'\n",
    "            heading_elements[heading].append(element)\n",
    "        else:\n",
    "            heading_elements[heading].append(element)\n",
    "\n",
    "    for kind, element_list in heading_elements.items():\n",
    "        parser = TOP_PARSERS.get(kind)\n",
    "        if parser is None:\n",
    "            print('no parser for', kind)\n",
    "            continue\n",
    "        yield (kind, parser(element_list))\n",
    "\n",
    "        \n",
    "def parse_top(session_number, top_element, top_type='normal'):\n",
    "    top_number = top_element.xpath('.//h2[@class=\"top-number\"]')[0].text_content()\n",
    "    if top_type == 'normal':\n",
    "        top_number = TOP_NUMBER_RE.search(top_number).group(1)\n",
    "    elif top_type == 'simple':\n",
    "        # Their internal representation of simple top numbers\n",
    "        top_number = '999' + TOP_SIMPLE_NUMBER_RE.search(top_number).group(1)\n",
    "   \n",
    "    title = top_element.xpath('.//div[@class=\"top-header-content-box\"]//a')[0].text_content()\n",
    "    data = {\n",
    "        'number': top_number,\n",
    "        'title': title,\n",
    "        'top_type': top_type\n",
    "    }\n",
    "    print(top_number, end=' ')\n",
    "    root = etree.fromstring(get(TOP_URL.format(num=session_number, top=top_number)))\n",
    "    top_details = dict(parse_top_detail(root))\n",
    "    data.update(top_details)\n",
    "    return data\n",
    "        \n",
    "\n",
    "TOP_SECTIONS = {\n",
    "    'Beschlüsse im vereinfachten Verfahren': 'simple'\n",
    "}\n",
    "\n",
    "def get_session_tops(session_number):\n",
    "    root = etree.fromstring(get(TO_URL.format(num=session_number)))\n",
    "    sections = root.xpath('.//div[@class=\"module type-1 tops\"]/div/*')\n",
    "    top_type = 'normal'\n",
    "    for section in sections:\n",
    "        if section.tag == 'ul':\n",
    "            top_elements = section.xpath('.//div[@class=\"top-header\"]')\n",
    "            for top_element in top_elements:\n",
    "                top = parse_top(session_number, top_element, top_type=top_type)\n",
    "                yield top\n",
    "        elif section.tag == 'h2':\n",
    "            section_heading = section.text_content()\n",
    "            top_type = TOP_SECTIONS[section_heading]\n",
    "        else:\n",
    "            raise Exception('Unexpected section tag {}'.format(section.tag))\n",
    "\n",
    "def get_session_details(sessions):\n",
    "    for session in sessions:\n",
    "        print('Session', session['number'])\n",
    "        session['tops'] = list(get_session_tops(session['number']))\n",
    "        print('\\n')\n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = list(get_sessions_this_year())\n",
    "sessions = get_session_details(sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 973\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25a 25b 25c 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 \n",
      "\n",
      "Session 972\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13a 13b 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53a 53b 53c 53d 53e 53f 54 55 56 57 58 59 60 61 62 63 64 65 66 \n",
      "\n",
      "Session 971\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43a 43b 43c 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 999a \n",
      "\n",
      "Session 970\n",
      "1a 1b 2 3 4 5 6a 6b 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34a 34b 34c 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70a 70b 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95a 95b 95c 96 97 98 99 100 101 102 103 104 105 106 107 108 999a 999b 999c 999d 999e 999f 999g 999h 999i 999j 999k 999l 999m 999n 999o 999p \n",
      "\n",
      "Session 969\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21a 21b 21c 21d 21e 22a 22b 23 24a 24b 25 26 27 28 29 30 31 32 33 34a 34b 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 999a 999b 999c 999d 999e 999f 999g 999h 999i 999j 999k 999l 999m 999n 999o 999p \n",
      "\n",
      "Session 968\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20a 20b 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37a 37b 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 999a 999b 999c 999d 999e \n",
      "\n",
      "Session 967\n",
      "1 2 3a 3b 4 5 6 7 8 9 10 11 12a 12b 13 14 15 16 17 18 19 20a 20b 21 22 23 24 25 26 27 28a 28b 28c 29 30 31 32 33 34 35 36a 36b 37 38 39 999a \n",
      "\n",
      "Session 966\n",
      "1 2 3 4 5 6 7 8 9 10a 10b 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32a 32b 33 34 35 36 37a 37b 38 39 40 41 42 999a \n",
      "\n",
      "Session 965\n",
      "1 2 3 4 5 6 7 8 9 10 11 12a 12b 13a 13b 13c 14a 14b 15 16 17 18 19 20 21 22 23 24 25 26 27 999a \n",
      "\n",
      "Session 964\n",
      "1 2a 2b 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 999a 999b 999c 999d 999e \n",
      "\n",
      "Session 963\n",
      "1 2 3 4 5 6 7a 7b 7c 7d 7e 8 9 10 11a 11b 11c 11d 12 13 14 15a 15b 15c 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 999a 999b 999c 999d \n",
      "\n",
      "Session 962\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 999a \n",
      "\n",
      "Session 961\n",
      "1 2 3 4 5a 5b 6 7 8 9 10 11 12 13 14 15 16 17 18 19a 19b 20 21a 21b 21c 21d 21e 22 23 24 25 999a 999b 999c 999d \n",
      "\n",
      "Session 960\n",
      "1 2 3 4 5a 5b 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21a 21b 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 999a 999b 999c 999d 999e \n",
      "\n",
      "Session 959\n",
      "1a 1b 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94a 94b 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 999a 999b 999c 999d 999e \n",
      "\n",
      "Session 958\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13a 13b 14 15 16 17 18 19 20a 20b 20c 20d 20e 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51a 51b 999a 999b \n",
      "\n",
      "Session 957\n",
      "1 2a 2b 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64a 64b 64c 64d 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 999a 999b 999c 999d 999e 999f \n",
      "\n",
      "Session 956\n",
      "1 2 3 4 5 6 7 8a 8b 9 10 11 12 13a 13b 13c 14 15 16 17 18a 18b 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 "
     ]
    }
   ],
   "source": [
    "sessions_archive = list(get_sessions_archive())\n",
    "sessions_archive = get_session_details(sessions_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sessions.json', 'w') as f:\n",
    "    json.dump(sessions, f)\n",
    "with open('sessions_archive.json', 'w') as f:\n",
    "    json.dump(sessions_archive, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
